# src/check_consistency.py
"""
This script performs a series of automated checks to verify the internal
consistency of the artifact. It ensures that required data files exist and that
the key numerical results generated by the pipeline match the claims made in
the paper.

It is intended to be run as the final step in the `run.sh` pipeline.
It will exit with a status code of 1 if any check fails.
"""
import sys
import json
from pathlib import Path
import pandas as pd
import numpy as np

# --- Configuration: Define paths and expected values ---

# Define base paths relative to the script location
PROJECT_ROOT = Path(__file__).resolve().parent.parent
REPORTS_DIR = PROJECT_ROOT / "reports"
DATA_DIR = PROJECT_ROOT / "data"

# A dictionary of files that MUST exist for the artifact to be valid.
REQUIRED_FILES = {
    "Ground-Truth Metrics (for Table 3)": DATA_DIR / "human_logs_derived" / "turn_level_metrics.csv",
    "LLM Replay (GPT-4.1)": PROJECT_ROOT / "llm_replay" / "gpt4.1_nano_subset.jsonl",
    "LLM Replay (Claude Sonnet 4)": PROJECT_ROOT / "llm_replay" / "claude_sonnet4_subset.jsonl",
    "Original Dataset Simulation Summary": REPORTS_DIR / "simulation_outputs" / "policy_summary_original.csv",
    "Statistical Analysis (for Table 2)": REPORTS_DIR / "statistical_analysis" / "table_2_policy_effect_ci.json",
    "Simulation Validation (for Table 3)": REPORTS_DIR / "statistical_analysis" / "table_3_simulation_validation.csv",
}

# Expected values from the paper's tables to verify against generated files.
PAPER_CLAIMS = {
    "table_1_static_baseline_synchrony": 0.079,
    "table_2": {
        "GPT-4.1 nano": {
            "stability": 0.131,
            "synchrony": -0.083,
            "coherence": -0.015,
        },
        "Claude Sonnet 4": {
            "stability": 0.163,
            "synchrony": -0.083,
            "coherence": 0.021,
        },
    },
}


# --- ANSI Color Codes for readable output ---
class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    ENDC = '\033[0m'

# --- Check Functions ---

def check_file_existence(errors: list) -> None:
    """Verifies that all critical data and result files exist."""
    print(f"{Colors.BLUE}--- Verifying critical file existence...{Colors.ENDC}")
    for name, path in REQUIRED_FILES.items():
        if not path.exists():
            status = f"{Colors.RED}[FAIL]{Colors.ENDC}"
            message = f"Missing required file: {path.relative_to(PROJECT_ROOT)}"
            errors.append(message)
        else:
            status = f"{Colors.GREEN}[PASS]{Colors.ENDC}"
            message = f"Found '{name}'"
        print(f"  {status} {message}")

def check_static_baseline_value(errors: list) -> None:
    """
    Verifies that the 'Static Baseline' synchrony value in the generated summary
    CSV matches the value claimed in the paper and used for plotting.
    """
    print(f"{Colors.BLUE}--- Verifying 'Static Baseline' synchrony value...{Colors.ENDC}")
    summary_path = REQUIRED_FILES["Original Dataset Simulation Summary"]
    if not summary_path.exists():
        print(f"  {Colors.YELLOW}[SKIP]{Colors.ENDC} Cannot check value because file is missing.")
        return

    try:
        df = pd.read_csv(summary_path)
        static_row = df[df['policy'] == 'Static Baseline']
        
        if static_row.empty:
            message = "Could not find 'Static Baseline' policy in policy_summary_original.csv"
            errors.append(message)
            print(f"  {Colors.RED}[FAIL]{Colors.ENDC} {message}")
            return

        actual_value = static_row['synchrony'].iloc[0]
        expected_value = PAPER_CLAIMS["table_1_static_baseline_synchrony"]

        if not np.isclose(actual_value, expected_value, atol=1e-3):
            message = f"'Static Baseline' synchrony mismatch. Found: {actual_value:.3f}, Expected: {expected_value:.3f}"
            errors.append(message)
            print(f"  {Colors.RED}[FAIL]{Colors.ENDC} {message}")
        else:
            print(f"  {Colors.GREEN}[PASS]{Colors.ENDC} Value matches paper's claim ({actual_value:.3f}).")

    except Exception as e:
        message = f"Could not read or parse {summary_path.name}: {e}"
        errors.append(message)
        print(f"  {Colors.RED}[FAIL]{Colors.ENDC} {message}")

def check_policy_effect_ci_means(errors: list) -> None:
    """
    Verifies that the mean delta values in the generated JSON for Table 2
    match the claims in the paper.
    """
    print(f"{Colors.BLUE}--- Verifying policy effect means (Table 2)...{Colors.ENDC}")
    ci_path = REQUIRED_FILES["Statistical Analysis (for Table 2)"]
    if not ci_path.exists():
        print(f"  {Colors.YELLOW}[SKIP]{Colors.ENDC} Cannot check values because file is missing.")
        return
    
    try:
        with open(ci_path, 'r') as f:
            data = json.load(f)
        
        claims = PAPER_CLAIMS["table_2"]
        mismatched = False
        
        for model_key, metrics in claims.items():
            json_model_data = data.get(model_key)
            
            if not json_model_data:
                message = f"Could not find exact model key '{model_key}' in {ci_path.name}"
                errors.append(message)
                print(f"  {Colors.RED}[FAIL]{Colors.ENDC} {message}")
                mismatched = True
                continue

            for metric, expected_mean in metrics.items():
                actual_mean = json_model_data.get(metric, {}).get("mean")
                
                if actual_mean is None:
                    message = f"Metric '{metric}' for model '{model_key}' not found in JSON."
                    errors.append(message)
                    mismatched = True
                elif not np.isclose(actual_mean, expected_mean, atol=1e-3):
                    message = f"Table 2 mean mismatch for {model_key}/{metric}. Found: {actual_mean:.3f}, Expected: {expected_mean:.3f}"
                    errors.append(message)
                    mismatched = True
        
        if not mismatched:
            print(f"  {Colors.GREEN}[PASS]{Colors.ENDC} All mean values in generated JSON match paper's claims.")
        else:
            print(f"  {Colors.RED}[FAIL]{Colors.ENDC} One or more mean values do not match paper claims. See details above.")

    except Exception as e:
        message = f"Could not read or parse {ci_path.name}: {e}"
        errors.append(message)
        print(f"  {Colors.RED}[FAIL]{Colors.ENDC} {message}")


def main():
    """Runs all consistency checks and reports a final summary."""
    errors = []
    
    print("\n=================================================")
    print("      Running Artifact Consistency Checks      ")
    print("=================================================\n")

    check_file_existence(errors)
    check_static_baseline_value(errors)
    check_policy_effect_ci_means(errors)

    print("\n----------------- Summary -----------------\n")
    if not errors:
        print(f"{Colors.GREEN}✅ ALL CONSISTENCY CHECKS PASSED.{Colors.ENDC}")
        print("   The artifact appears to be internally consistent and matches the paper's key results.")
        sys.exit(0)
    else:
        print(f"{Colors.RED}❌ {len(errors)} CONSISTENCY CHECK(S) FAILED.{Colors.ENDC}")
        print("   Please review the errors listed below:")
        for i, error in enumerate(errors, 1):
            print(f"   {i}. {error}")
        sys.exit(1)

if __name__ == "__main__":
    main()